{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a052a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 Definzione funzioni di processamento dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzioni di esplorazione dei dati testuali e non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a573325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloudNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for wordcloud from https://files.pythonhosted.org/packages/f5/b0/247159f61c5d5d6647171bef84430b7efad4db504f0229674024f3a4f7f2/wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl (300 kB)\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.2 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/300.2 kB 330.3 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 51.2/300.2 kB 327.7 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 133.1/300.2 kB 718.6 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 204.8/300.2 kB 958.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 300.2/300.2 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09d7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblobNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for textblob from https://files.pythonhosted.org/packages/02/07/5fd2945356dd839974d3a25de8a142dc37293c21315729a41e775b5f3569/textblob-0.18.0.post0-py3-none-any.whl.metadata\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/626.3 kB 435.7 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 573.4 kB/s eta 0:00:01\n",
      "   -------- ----------------------------- 143.4/626.3 kB 774.0 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 204.8/626.3 kB 888.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 256.0/626.3 kB 983.0 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 327.7/626.3 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 389.1/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 440.3/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 491.5/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 532.5/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 593.9/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 626.3/626.3 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b5b06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Assicurati di scaricare le risorse necessarie di nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "def remove_null_columns(df):\n",
    "    return df.dropna(axis=1, how='any')\n",
    "\n",
    "def display_basic_info(df, target):\n",
    "    print(\"=== Informazioni di base sul dataset ===\")\n",
    "    print(df.head())\n",
    "    print(\"\\n=== Statistiche descrittive ===\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n=== Informazioni sulle colonne ===\")\n",
    "    print(df.info())\n",
    "    print(\"\\n=== Conteggio dei valori per la colonna 'target' ===\")\n",
    "    print(df[target].value_counts())\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def analyze_text_length(data, text_column=None):\n",
    "    print(\"=== Analisi della lunghezza dei testi ===\")\n",
    "    if text_column:\n",
    "        text_lengths = data[text_column].apply(len)\n",
    "        word_counts = data[text_column].apply(lambda x: len(x.split()))\n",
    "    else:\n",
    "        text_lengths = [len(text) for text in data]\n",
    "        word_counts = [len(text.split()) for text in data]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(text_lengths, bins=50)\n",
    "    plt.title('Distribuzione della lunghezza dei testi')\n",
    "    plt.xlabel('Lunghezza del testo')\n",
    "    plt.ylabel('Frequenza')\n",
    "    plt.show()\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def most_common_words(data, text_column=None):\n",
    "    print(\"=== Parole pi첫 comuni nel testo ===\")\n",
    "    if text_column:\n",
    "        all_words = ' '.join(data[text_column]).split()\n",
    "    else:\n",
    "        all_words = ' '.join(data).split()\n",
    "        \n",
    "    common_words = Counter(all_words).most_common(10)\n",
    "    print(\"Parole pi첫 comuni:\", common_words)\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def generate_wordcloud(data, text_column=None):\n",
    "    print(\"=== Word Cloud delle parole pi첫 comuni ===\")\n",
    "    if text_column:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data[text_column]))\n",
    "    else:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud delle parole pi첫 comuni')\n",
    "    plt.show()\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def tokenize_and_remove_stopwords(data, text_column=None):\n",
    "    print(\"=== Tokenizzazione e rimozione delle stop words ===\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    if text_column:\n",
    "        tokens = data[text_column].apply(word_tokenize)\n",
    "        tokens_no_stop = tokens.apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "    else:\n",
    "        tokens = [word_tokenize(text) for text in data]\n",
    "        tokens_no_stop = [[word for word in token if word.lower() not in stop_words] for token in tokens]\n",
    "    \n",
    "    if text_column:\n",
    "        all_tokens = [word for tokens in tokens_no_stop for word in tokens]\n",
    "    else:\n",
    "        all_tokens = [word for text_tokens in tokens_no_stop for tokens in text_tokens for word in tokens]\n",
    "    \n",
    "    common_tokens = Counter(all_tokens).most_common(10)\n",
    "    print(\"Token pi첫 comuni senza stop words:\", common_tokens)\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def sentiment_analysis(data, text_column=None):\n",
    "    print(\"=== Analisi del sentiment ===\")\n",
    "    if text_column:\n",
    "        sentiments = data[text_column].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    else:\n",
    "        sentiments = [TextBlob(text).sentiment.polarity for text in data]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(sentiments, bins=50)\n",
    "    plt.title('Distribuzione del sentiment')\n",
    "    plt.xlabel('Valore del sentiment')\n",
    "    plt.ylabel('Frequenza')\n",
    "    plt.show()\n",
    "    print(\"\\n======================================\\n\")\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    return list(nltk.ngrams(tokens, n))\n",
    "\n",
    "def extract_ngrams(data, text_column=None, n=2):\n",
    "    print(f\"=== Estrazione di {n}-grammi ===\")\n",
    "    if text_column:\n",
    "        ngrams_list = data[text_column].apply(lambda x: get_ngrams(x, n))\n",
    "    else:\n",
    "        ngrams_list = [get_ngrams(text, n) for text in data]\n",
    "    \n",
    "    print(f\"Estrazione completata per {n}-grammi.\")\n",
    "    print(\"\\n======================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bed3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzioni di ingegnerizzazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c311748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def remove_null_columns(df):\n",
    "    return df.dropna(axis=1, how='any')\n",
    "\n",
    "def create_tfidf_features(df, text_column):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(df[text_column])\n",
    "    return X_tfidf, vectorizer\n",
    "\n",
    "def select_k_best_features(X, y, k=10000):\n",
    "    selector = SelectKBest(score_func=chi2, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    return X_selected, selector\n",
    "\n",
    "def standardize_features(X):\n",
    "    scaler = StandardScaler(with_mean=False)  # with_mean=False poich챕 i dati sono sparsi\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Dataset dei triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a7410f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Informazioni di base sul dataset ===\n",
      "     category classification           triggerTitle  \\\n",
      "0  appliances         public           End of cycle   \n",
      "1  appliances         public      10 Pods remaining   \n",
      "2  appliances      available   Rinse aid levels low   \n",
      "3  appliances      available  Filter needs cleaning   \n",
      "4  appliances      available          Leak detected   \n",
      "\n",
      "                                         triggerDesc  \\\n",
      "0  This Trigger fires every time your GE Dishwash...   \n",
      "1  This Trigger fires every time your GE Dishwash...   \n",
      "2  This Trigger fires when your GE Dishwasher sen...   \n",
      "3  This Trigger fires every time your GE Dishwash...   \n",
      "4  This Trigger fires when your GE Dishwasher det...   \n",
      "\n",
      "         triggerChannelName  triggerChannelId  \\\n",
      "0  GE Appliances Dishwasher          14669108   \n",
      "1  GE Appliances Dishwasher          14669108   \n",
      "2  GE Appliances Dishwasher          14669108   \n",
      "3  GE Appliances Dishwasher          14669108   \n",
      "4  GE Appliances Dishwasher          14669108   \n",
      "\n",
      "                            triggerChannelUrl    triggerFieldList  \\\n",
      "0  https://ifttt.com/ge_appliances_dishwasher  [Which appliance?]   \n",
      "1  https://ifttt.com/ge_appliances_dishwasher  [Which appliance?]   \n",
      "2  https://ifttt.com/ge_appliances_dishwasher  [Which appliance?]   \n",
      "3  https://ifttt.com/ge_appliances_dishwasher  [Which appliance?]   \n",
      "4  https://ifttt.com/ge_appliances_dishwasher  [Which appliance?]   \n",
      "\n",
      "  triggerIngList  \n",
      "0             []  \n",
      "1             []  \n",
      "2             []  \n",
      "3             []  \n",
      "4             []  \n",
      "\n",
      "=== Statistiche descrittive ===\n",
      "       triggerChannelId\n",
      "count      1.426000e+03\n",
      "mean       8.756037e+08\n",
      "std        6.606896e+08\n",
      "min        1.000000e+00\n",
      "25%        1.995600e+08\n",
      "50%        8.912792e+08\n",
      "75%        1.352861e+09\n",
      "max        2.143871e+09\n",
      "\n",
      "=== Informazioni sulle colonne ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1426 entries, 0 to 1425\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   category            1426 non-null   object\n",
      " 1   classification      1426 non-null   object\n",
      " 2   triggerTitle        1426 non-null   object\n",
      " 3   triggerDesc         1426 non-null   object\n",
      " 4   triggerChannelName  1426 non-null   object\n",
      " 5   triggerChannelId    1426 non-null   int64 \n",
      " 6   triggerChannelUrl   1426 non-null   object\n",
      " 7   triggerFieldList    1426 non-null   object\n",
      " 8   triggerIngList      1426 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 100.4+ KB\n",
      "None\n",
      "\n",
      "=== Conteggio dei valori per la colonna 'target' ===\n",
      "classification\n",
      "public       813\n",
      "private      394\n",
      "available    219\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================\n",
      "\n",
      "=== Analisi della lunghezza dei testi ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26972\\89266600.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_null_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay_basic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"classification\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0manalyze_text_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#most_common_words(df, 'text_column')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#generate_wordcloud(df, 'text_column')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#tokenize_and_remove_stopwords(df, 'text_column')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26972\\3049948960.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, text_column)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0manalyze_text_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=== Analisi della lunghezza dei testi ===\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtext_column\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtext_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mtext_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10030\u001b[0m             \u001b[0mby_row\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10031\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10032\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10033\u001b[0m         )\n\u001b[1;32m> 10034\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"apply\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[1;31m# raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mode.chained_assignment\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m                     \u001b[1;31m#  series_generator will swap out the underlying data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26972\\3049948960.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "file_path = 'Dataset/classificationTriggers.json'  # Inserisci il percorso del tuo dataset JSON\n",
    "df = load_data(file_path)\n",
    "\n",
    "df = remove_null_columns(df)\n",
    "display_basic_info(df, \"classification\")\n",
    "analyze_text_length(df, df.columns.tolist())\n",
    "#most_common_words(df, 'text_column')\n",
    "#generate_wordcloud(df, 'text_column')\n",
    "#tokenize_and_remove_stopwords(df, 'text_column')\n",
    "#sentiment_analysis(df, 'text_column')\n",
    "#extract_ngrams(df, 'text_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413abd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
